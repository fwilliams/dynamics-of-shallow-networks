\newpage
\newpage

\section{Practical Implications and Experiments}
\subsection{Outline}
\note{
\begin{itemize}
    \item Verify trajectories match: 2 examples
    \item Delta mostly constant for GD (Appendix)
    \item Cubic spline
    \item 3D plots showing PL vs CS
    \item Same initialization in function space with different initialization has very different dynamics
\end{itemize}
}

In this section we present experimental results illustrating lazy and non-lazy training as well as the robustness to noise arising from flat minima.

\subsection{Preventing Noise Overfitting}
In the over-parameterized regime, a full rank kernel matrix produces interpolatory solutions. In the presence of noisy data, these solutions may be undesirable. Since knots in high curvature regions move faster initially (\todo{better way of saying this}), we can fit features in the data by limiting the number of iterations. Figure~\ref{fig:early-stopping} demonstrates the result for both the tangent and adaptive kernels. 

% {figures/nonlazy-square-noise.png}
\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{figures/lazy-square-noise.png}
    \includegraphics[width=0.45\textwidth]{figures/nonlazy-square-noise.png}
    \caption{The result of fitting a network with 2000 neurons to a square wave with gaussian noise added. Each network was fit with 10k iterations of gradient descent with Nesterov momentum. The left image shows the lazy training regime and the right shows the adaptive kernel regime.}
    \label{fig:early-stopping}
\end{figure}

Note however, that for a fixed kernel, regularization of the least squares problem will yields results similar to gradient descent with early stopping.

A natural consequence of considering only the dynamics of the kernel during training is that once a kernel has been fixed, we can control the smoothness and interpolation of the solution by tuning a single parameter and solving a linear system. We can thus control the quality of the model without retraining the network. 

Figure~\ref{fig:lsq-lazy} shows the result of varying regularization using the tangent kernel and Figure~\ref{fig:lsq-non-lazy} shows the result of regularization using the adaptive kernel.


\begin{figure}
    \centering
    \includegraphics[width=0.32\textwidth]{figures/lazy-square-noise-lsq-1e0.png}
    \includegraphics[width=0.32\textwidth]{figures/lazy-square-noise-lsq-1e-1.png}
    \includegraphics[width=0.32\textwidth]{figures/lazy-square-noise-lsq-1e-2.png}
    \caption{Regularized least squares kernel regression for the lazy kernel. We use different regularization factors (left: 1, mid: 0.1, right: 0.01) to control the smoothness of the solution.}
    \label{fig:lsq-lazy}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=0.32\textwidth]{figures/nonlazy-square-noise-lsq-1e0.png}
    \includegraphics[width=0.32\textwidth]{figures/nonlazy-square-noise-lsq-1e-1.png}
    \includegraphics[width=0.32\textwidth]{figures/nonlazy-square-noise-lsq-1e-2.png}
    \caption{Regularized least squares kernel regression for the non-lazy kernel. We use different regularization factors (left: 1, mid: 0.1, right: 0.01) to controll the smoothness of the solution.}
    \label{fig:lsq-non-lazy}
\end{figure}

\section{Global Training Dynamics}
% The invariants $\delta_i$ allow us to present a concrete framework for understanding the global dynamics of neurons in terms of their initialization. We can understand the global dynamics of neural networks in terms of two regimes corresponding to eigenvectors of $M_\delta$ in the previous section: \emph{Radial training} and \emph{Tangential training}. 

In this section, we consider the extreme cases of initialization: $\delta \ll -\|\xi\|$ (radial dynamics) and $\delta \gg \|\xi\|$ (tangential dynamics). This corresponds to training only the bottom and only the top layer of the network, respectively. 

% We show that lazy training occurs when $\delta \ll 0$ (radial motion in the reduced parameters). We demonstrate that in this regime, the network is biased towards ``smooth'' solutions. At the other end of the spectrum \cite{maennel2018gradient} observe that small initializations lead to a quantization effect of the weight vectors independent of the size of the network. This phenomenon occurs more prominently as $\delta$ grows larger. Furthermore, we describe precisely where the the weight vectors concentrate in terms of the input samples. We also note that the samples at which neurons concentrate are not fixed, but are dynamical quantities which depend on the activation pattern and on the residuals.

% We note that scaling the gradient flow is equivalent to scaling the distributions from which the initial parameters, $\theta$ are sampled, thus we can control the value of $\delta$ by either rescaling the gradient flow or changing the initial distribution of $\theta$.


\subsection{Radial Training}

\note{We should say that radial training is what happens with the standard initialization using pytorch}

In pure radial regime, we are only learning the outer layer parameters and leaving the inner parameters fixed. This corresponds to \emph{kernel learning}~\note{Cite someone}. In the overparameterized setting, we wish to solve

\begin{equation}\label{eq:lsq_overparameterized}
\begin{gathered} 
    \text{minimize } ||c||^2\\
    \text{subject to } M_x(a, b) c = y
\end{gathered}
\end{equation}

which is strictly convex with convex constraints. Writing $M = M_x(a,b)$, we have that solution to \eqref{eq:lsq_overparameterized} is given by $c = M^T (M M^T)^{-1} y$. We define the kernel

\begin{equation}
    K_{(a,b)}(x,x') = \sum_{i=1}^m [a_i x + b_i]_+[a_i x' + b_i]_+
\end{equation}


The optimized function is given by

\begin{equation}
    f(x) = \sum_{j=1}^s v_j K(x_j, x)
\end{equation}


where $v = K(x_i,x_j)^{-1} y$. Of course, this closed form solution only corresponds to the point of convergence of the gradient flow only if $c(0) = 0$. The more general dynamics of the flow can be similarly derived in terms of the kernel $K$ (see Appendix). Furthermore, it is known in this setting that early stopping for
gradient descent will act as a regularizes as these eigenfunctions of $K$ typically correspond to lower frequency features of the function being fit \note{cite}.







\paragraph{Infinite width limit}
% We analyze the behavior of the \emph{limit kernel} as the number of weights $m \rightarrow \infty$ in the radial regime. We show that the overparameterized least squares problem in this setting minimizes curvature. Furthermore, for 1D functions, the network functions are natural cubic spines. 
In the infinite width ($m = \infty)$ limit, the parameters, $\theta$ become densities $\theta(s) = (a(s), b(s), c(s))$. Assuming that $s \in [k_1, k_2], x \in [k_1, k_2]$, then we can write the function $f$ as

\begin{equation}
    f(x) = \int_{k_1}^{k_2} c(s) [a(s) x + b(s)]_+ ds
\end{equation}

and the least squares problem becomes
\begin{equation}
    \begin{gathered}
    \text{minimize } ||c(s)||^2\\
    \text{subject to } \int_{k_1}^{k_2} c(s) [a(s)x_i + b(s)]_+ ds = y_i
    \end{gathered}
\end{equation}

\begin{lemma}\label{le:curvature}
The second derivative of $f(x)$ is the parameter density $c(s)$. i.e. 
\begin{equation*}\partial_x^2 f(x) = c(s)\end{equation*}
\end{lemma}
\begin{proof}
\todo{TODO}
\end{proof}

Thus, the quantity $||c(s)||^2 = ||\partial_x^2 f(x)||^2$ being minimized is the \emph{curvature}. Thus, we can view the radial regime as an implicit bias towards ``smooth'' functions.



For 1D functions with knots initialized from a uniform distribution, we give an exact equation for the limit kernel. 

\begin{lemma}\label{le:cubic_spline}
Assume that the knots $e(s) =\ -\frac{b(s)}{a(s)} ~ U(k_1, k_2)$, then the inifinite width kernel, $K(x, y)$ is a cubic polynomial in $x$ and $y$
\end{lemma}
\begin{proof}

We can rewrite the kernel,
\begin{equation}
    K(x, y) = \int_{k_1}^{k_2} [a(s)x + b(s)]_+ [a(s)y + b(s)]_+ ds
\end{equation}

in terms of the knots $e(s)$

\begin{equation}
    K(x, y) = \int_{k_1}^{k_2} \tau(x, s) \tau(y, s) a(s)^2 (x - e(s)) (y - e(s)) ds
\end{equation}

where $\tau(x, s) = \mathds{1}[a(s)x + b(s) \geq 0]$. $a(s)$. Sicne the knots $e(s) = -\frac{b(s)}{a(s)}$ are uniformly distributed, we can write $e(s) = s$ which implies $-b(s) = s a(s)$. In this case, we can assume without loss of generality that $v(s) = -s$ and $u(s) = 1$. We then rewrite the kernel

\begin{equation}
    \begin{aligned}
    K(x, y) &= \int_{k_1}^{k_2} \tau(x, s) \tau(y, s) (x - s) (y - s) ds\\
            &= \int_{k_1}^{\min(x, y)} (x - s) (y - s) ds\\ 
            &= s xy - \frac{1}{2} s^2 (x + y) - \frac{1}{3} s^3 \bigg\rvert_{k_1}^{\min(x, y)}
    \end{aligned}
\end{equation}

which is a cubic function in $x$ and $y$.
\end{proof}

Recall that for full rank $M_x(a,b)$, the function $f(x)$ interpolates the samples, $x_i$. Thus, a corollary to Lemmas \ref{le:curvature} and \ref{le:cubic_spline} is that for uniform knot initialization, 1D neural network functions are cubic splines interpolating the samples. Figure~\ref{fig:cubic_splines} shows the convergence to a cubic spline as the number of neurons grows large. \note{We need to cite the paper on linear splines here.} 

\begin{figure}\label{fig:cubic_splines}
    \centering
    \minipage{0.33\textwidth}
    \includegraphics[width=\linewidth]{figures/cubic_spline100.pdf}
    \endminipage\hfill
    \minipage{0.33\textwidth}
    \includegraphics[width=\linewidth]{figures/cubic_spline_1k.pdf}
    \endminipage\hfill
    \minipage{0.33\textwidth}
    \includegraphics[width=\linewidth]{figures/cubic_spline_10k.pdf}
    \endminipage
    \caption{Regression in the purely tangential regime ($\delta = -\infty$) using varying knots. We observe that as the number of knots increases, the function, $f_\theta$ converges to a cubic spline interpolant with $\partial_x^2 = 0$ on the boundary.}
    \label{fig:radial_trajectores}
\end{figure}



















\subsection{Tangential Training, $\delta \gg \|\xi\|$}


\note{Use interesting?}

Recall that Lemma~\ref{le:pw_continuous} states that the gradient field $\nabla \tilde{L}$ is piecewise continuous. An important consequence of this result is that the gradient field could point in opposite directions on the boundary of a sample (\todo{Figure \ref{fig:todo}}). In this case, a neuron could get stuck along the boundary for that sample and only be able to move radially. We call the samples at which neurons get stuck \emph{attractive}. The existence of attractive samples leads to neurons piling up at samples, biasing the functions $f(x)$ to be piecewise linear with a small number of pieces.

We now give a precise condition for when a sample is attractive. We also show that the set of attractive samples is a dynamical quantity which depends only on the activation.

Recall that in the reduced parameter space, a sample $x_i$ corresponds to a line in the $(u, v)$ plane. The sample $x_i$ is \emph{attractive} if the loss gradient in the reduced parameters $\nabla \tilde{L}$ points in opposite directions on either side of this line. 

Formally, we can express attractiveness as a condition on the time derivative of the knots $e_i(t)$. 

\begin{equation}\label{eq:moving_knots}
\partial_t e_i(t) = \frac{-a_i \partial b_i + b_i \partial a_i}{a_i^2} = c_i\frac{- a_i \sum_
{j=1}^s
\tau_{ij} r_j + b_i \sum_
{j=1}^s \tau_{ij} x_j r_j}{a_i^2} = \frac{\sum_{j=1}^s \tau_{ij}c_i(-a_i
+x_j b_i)r_j}{a_i^2}
\end{equation}

\note{Possibly useful to frame attractiveness/interestingness:}
\begin{definition}
Consider a function of the form $h(\bm w) = \sum_{i=1}^s r_i [\langle \bm w,\bm x_i\rangle ]_+$ with $\bm w, \bm x_i \in \RR^2$ and $r_i \in \RR$. We say that $\hat{ \bm w} \in \mathbb S^1$ is a \emph{stable direction} if $\hat{ \bm w} \in \alpha \partial h(\hat {\bm w})$ for some $\alpha > 0$ (perhaps give more intuitive definition/picture?).
\end{definition}

A stable direction can either be on the boundary or on the internal part of a segment. From~\eqref{eq:moving_knots}, we are interested in the stable directions for $h((a_i,b_i)) = \epsilon_i \sum_{i=1}^s r_j [(-a_i + x_jb_j)]_+$. Stable directions on the boundaries are ``interesting points''.


\note{Think about how to present this}

Assume that the samples $x_1 < x_2 < \ldots < x_s$ are ordered and suppose that the knot $e_i$ lies on the sample $x_k$, i.e. $e_i(t) = x_k$. Denoting $r = (r_j(\theta))_{j=1}^s$ as the vector of residuals, we have that a the sample $x_k$ is attractive if

\begin{equation}
\begin{aligned}
    &\sum_{j=1}^{k-1} (1+x_j x_k) r_j \lessgtr 0 \quad \mbox{ and } \qquad \sum_{j=1}^k (1+x_j x_k) r_j \gtrless 0, \quad \mbox{ or }\\
    &\sum_{j=k}^s (1+x_j x_k) r_j \gtrless 0 \quad \mbox{ and } \qquad \sum_{j=k+1}^{s} (1+x_j x_k) r_j \lessgtr 0.\\
\end{aligned}
\end{equation}

If $z_j = (1,x_j) r_j$, we can rewrite this as

\begin{equation}
\begin{aligned}\label{eq:attractor_cond}
    &\sum_{j=1}^{k-1} \langle z_k , z_j \rangle < 0 \quad \mbox{ and } \qquad \sum_{j=1}^{k} \langle z_k, z_j \rangle > 0, \quad \mbox{ or }\\
    &\sum_{j=k}^{s} \langle z_k , z_j \rangle > 0 \quad \mbox{ and } \qquad \sum_{j=k+1}^{s} \langle z_k, z_j \rangle < 0.\\
\end{aligned}
\end{equation}


This can be written in terms of the Gram matrix $Z_{ij} = (\langle z_i, z_j\rangle)$.

\paragraph{Dynamics of Attractive Samples}

The attractiveness condition \eqref{eq:attractor_cond} is a dynamic quantity depending on the residuals $r(\theta)$ as well as the samples, $x$. A sample can only change from attractive to unattractive or vice-versa when the activation matrix $\tau$ changes.

\begin{lemma}
If the truth value of the condition \eqref{eq:attractor_cond} changes at time $t$, then the activation matrix, $\tau$ must also change at time $t$.
\end{lemma}
\begin{proof}
\todo{TODO}
\end{proof}

Qualitatively, a sample is attractive when there is a large change in sign between the average residual on one side of the sample and the sample itself. Attractive samples on the ''peaks`` and ``valleys of the of the residual. As with the residual case, low frequency features in the data tend to be fit first with high frequency features being fit later. \todo{Figure~\ref{fig:todo}} shows an example of this behavior.



























\iffalse
\subsection{Old}
% It is easy to verify that the eigenvalue-eigenvector pairs of $K(\xi)$ are
% $(\beta a^2+ \beta b^2 + \alpha c^2, (a,b))$ and $(\alpha c^2, (-b,a))$, which correspond to radial and tangential components of $(u, v)$. From this we deduce that


% \begin{itemize}
%     \item If $\alpha c^2 \ll \beta (a^2 + b^2)$, which means that $\delta <
%     0$, then $\xi_i'(t) \propto (a(t),b(t))$, and the reduced neuron
%     moves~\emph{radially}.
%     \item If $\alpha c^2 \gg \beta (a^2 + b^2)$, which means that $\delta > 0$, then
%     $\xi_i'(t) \propto \nabla^\xi(\xi)_i$, so the reduced neuron follows themaybePres
% \end{itemize}



\paragraph{Node dynamics.}

\begin{equation}
\dot e_i(t) = \frac{-\dot b_i a_i + \dot a_i b_i}{a_i^2} = c_i\frac{- a_i \sum_
{j=1}^s
\tau_{ij} r_j + b_i \sum_
{j=1}^s \tau_{ij} x_j r_j}{a_i^2} = \frac{\sum_{j=1}^s \tau_{ij}c_i(-a_i
+x_j b_i)r_j}{a_i^2}
\end{equation}

If $e_i(t) = x_k$, that is, $-b_i = x_k a_i$, then

\begin{equation}
\dot e_i(t) = -\frac{c_i}{a_i}\sum_{j=1}^s \tau_{ij} (1
+ x_j x_k)r_j
\end{equation}

Assuming $x_1 < \ldots < x_s$, we say that a sample point $x_k$ is \emph{attractive} for a residual vector $r$ if

\begin{equation}
\begin{aligned}
    &\sum_{j=1}^{k-1} (1+x_j x_k) r_j \lessgtr 0 \quad \mbox{ and } \qquad \sum_{j=1}^k (1+x_j x_k) r_j \gtrless 0, \quad \mbox{ or }\\
    &\sum_{j=k}^s (1+x_j x_k) r_j \gtrless 0 \quad \mbox{ and } \qquad \sum_{j=k+1}^{s} (1+x_j x_k) r_j \lessgtr 0.\\
\end{aligned}
\end{equation}

If $z_j = (1,x_j) r_j$, we can rewrite this as

\begin{equation}
\begin{aligned}\label{eq:attractor_cond}
    &\sum_{j=1}^{k-1} \langle z_k , z_j \rangle < 0 \quad \mbox{ and } \qquad \sum_{j=1}^{k} \langle z_k, z_j \rangle > 0, \quad \mbox{ or }\\
    &\sum_{j=k}^{s} \langle z_k , z_j \rangle > 0 \quad \mbox{ and } \qquad \sum_{j=k+1}^{s} \langle z_k, z_j \rangle < 0.\\
\end{aligned}
\end{equation}


This can be written in terms of the Gram matrix $Z_{ij} = (\langle z_i, z_j\rangle)$.




\subsection{Denis}
It is best to write $K$ as 
\begin{equation}
c(||\xi||^2)^2 I +  c(||\xi||^2)^{-2}  \xi \xi^T
\end{equation} 

Denoting, $c(||\xi||^2)^2$ as $c^2$, the equation has the form  
\begin{equation}
    \xi'(t) =  c^2 w_0 + c^{-2} \xi (\xi^T w_0)
\end{equation}

Then
\begin{equation}
\begin{aligned}
||\xi||^{2\prime} = 2 \xi'^T \xi &= 2(c^2 w_0 + c^{-2} \xi (\xi^T w_0))^T \xi\\
                          &= 2(c^2 (\xi^T w_0) + c^{-2} (\xi^T w_0) \xi^T \xi)\\
                          &= 2(c^2 (\xi^T w_0) + c^{-2} (\xi^T w_0) ||\xi||^2)
\end{aligned}
\end{equation}

Introducing new variables $x = (\xi^T w_0)$ and $y = ||\xi||^2$, we get
\begin{equation}
\begin{aligned}
x' &= w_0^T \xi' = c^2 ||w_0||^2 + c^{-2} x^2\\
y' &= 2 c^2x + 2 c^{-2} x y 
\end{aligned}
\end{equation}

Now assume $||w_0||^2 = 1$ and eliminate $dt$, to get
\begin{equation}
  \begin{aligned}
  \frac{dx}{dy} &= \frac{c^2 + c^{-2}x^2}{c^2x + c^{-2} x y}\\
                &= \frac{c^4 + x^2}{x (c^4 + y)}
  \end{aligned}
\end{equation}

We can then rearange the equation:
\begin{equation}
  \begin{aligned}
  \frac{(dx) x}{dy} &= \frac{c^4 + x^2}{2(c^4 + y)}\\
  \frac{dx^2}{dy} &= \frac{c^4 + x^2}{c^4 + y}
  \end{aligned}
\end{equation}

And letting $z = x^2$, we get

\begin{equation}
    \frac{dz}{dy} = \frac{c^4 + z}{c^4 + y} 
\end{equation}

% \begin{equation}
%     \begin{aligned}
%     &\frac{b x^2 + y^2}{2 y (b x^2 + x)}\\
%     &\frac{b y^2 + x^2}{2 x (b y^2 + y)}\\
%     \end{aligned}
% \end{equation}


\subsection{Interpretation of attractor points}
We can rewrite the first condition of \eqref{eq:attractor_cond} as 

\begin{equation}
\begin{aligned}\label{eq:attractor_cond_interp}
    -(1 + x_k^2) r_k^2 &< \sum_{j=1}^{k-1} (1 + x_k x_j) r_k r_j \Rightarrow\\
    \langle z^\prime_k, z^\prime_k \rangle r_k^2 &< \sum_{j=1}^{k-1} \langle z^\prime_j, z^\prime_k \rangle r_k r_j
    % -1 < \sum_{j=1}^{k-1} \frac{1 + x_j x_j}{1 + x_k^s} \frac{r_j}{r_k} \Rightarrow\\
    % -1 < \sum_{j=1}^{k-1} \frac{\langle z^\prime_j, z^\prime_k \rangle}{\langle z^\prime_k, \z^\prime_k \rangle} \frac{r_j}{r_k}
\end{aligned}
\end{equation}

Where $z^\prime_l = (1, x_l)$. The weight terms, $\langle z^\prime_j, z^\prime_k \rangle$ have a geometric interpretation as projections of the vector $z^\prime_j$ onto $z^\prime_k$, and thus (assuming all $x_i > 0$), are larger for nodes farther away from $x_k$. Furthermore since all the $x_i$ have the same sign, \eqref{eq:attractor_cond_interp} will hold when the residual at $x_k$ differs greatly from the weighted average (using the weight terms above) of the previous residuals (and also changes sign). 

\note{This part is conjecture that I'm pretty sure is true and we can show by deriving the eigenfunctions of the spline kernel}
At initialization, the kernel, $K$ is an approximation of the limiting kernel with 
\fi


\section{Lazy training}
\subsection{Outline}
\begin{itemize}
    \item Family of kernels \note{Matthew}
    \item Splines
    \item Dynamics with eigenvectors (think about uniform distribution).\note{Francis}
\end{itemize}

\subsection{Families of kernels}

In the ``lazy'' regime, training the network reduces to kernel learning. We recall that given

\begin{equation}
f(x) = \int_D \varphi(x,\theta) c(\theta) d\theta,
\end{equation}

the solutions least-squares interpolation

\begin{equation}\label{eq:4-1-min}
\min \int \|c(\theta)\|^2 d\theta \quad \mbox{ s.t. } f(x_i) = y_i, \,\, i=1,\ldots,s.
\end{equation}

can be written in terms of the kernel $K(x,x') = \int \varphi(x,\theta) \varphi(x',\theta) d\theta$. Indeed, we have that 

\begin{equation}
    \hat f(x) = \int_D \hat c(\theta) \varphi(x,\theta) d\theta = \sum_{i=0}^n v_i \int_D \varphi(x,\theta)\varphi(x_i,\theta) d\theta = \sum_{i=0}^n v_i K(x,x_i).
\end{equation}

where $v = K(x_i,x_j)^{-1} y$. In our setting, we can express a 1D ReLU network function in the form

\begin{equation}
f(x) = \int c_+(t)[a_+(t) x - ta_+(t)]_+ dt + \int c_-(t)[a_-(t) x - ta_-(t)]_+ dt,
\end{equation}

where $t$ represents the knot $e = b/a$. The functions $a_+(t)$ and $a_{-}(t)$ encode the distribution of neurons
with positive and negative coefficient $a$, respectively. The corresponding kernel depends on the function $a_+,a_-$, and can be written as

\begin{equation}
\begin{aligned}
K_a(x_1,x_2) &= \int [a(t) x_1 - a(t)t]_+ [a(t) x_2 - a(t)t]_+ dt\\
&= \int_{t_0}^{\min(x_1,x_2)} a_+(t)^2 (x_1 - t) (x_2 - t)
dt + \int_{\max(x_1,x_2)}^{t_1} a_{-}(t)^2 (x_1 - t) (x_2 - t) dt\\
\end{aligned}
\end{equation}

\begin{proposition} If $a_+(t) = \mathds 1[t_0,t_1]$ and $a_-(t) = 0$ (infinite uniformly distributed neurons with $a=1$), then
\[
K(x, x') = \frac{1}{6} \, {\left(3 \, x' -2 \, t_0 - x\right)} {\left(x-t_0\right)}^{2}, \qquad x \le x'.
\]
If $a_+(t) = \mathds 1[t_0,t_1]$ and  $a_-(t) = \mathds 1[t_0,t_1]$ (infinite uniformly distributed neurons with $a=1$ and $a=-1$), then
\[
K(x, x') = \frac{1}{6} \, {\left(3 \, x' - 2 \, a - x \right)} {\left(x - t_0\right)}^{2} + \frac{1}{6} \, {\left(2 \, t_1 - 3 \, x + x'\right)} {\left(b - x'\right)}^{2}, \qquad x \le x'.
\]
\end{proposition}

\note{Discuss relation with known ReLU kernels (different distribution) and with recent 1D ReLU paper}.

\begin{corollary}
If $a_+(t) = \mathds 1[t_0,t_1]$ and $a_-(t) = 0$ or $a_+(t) = \mathds 1
[t_0,t_1]$ and $a_-(t) = 1[t_0,t_1]$, the solution to least squares interpolation is a~\emph{cubic spline}, that is, a $C^2$ curve that is cubic in every interval $[x_i,x_j]$.
\end{corollary}

We include the following derivation is similar to the one in \cite{NTKJacot}, but using a finite dimensional Kernel.

\subsection{Implicit Regularization of Kernel Least-Squares}
Consider the least-squares problem:

\begin{equation}\label{eq:lsq}
\begin{gathered}
    \text{minimize } \frac{1}{2} ||Mc - y||^2
\end{gathered}
\end{equation}

If we solve the problem \eqref{eq:lsq} with gradient descent, then $c$ follows the ODE:
\begin{align}
    \partial_t c(t) &= - \nabla \frac{1}{2} ||Mc(t) - y||^2\\
                    &= -(M^T M c - M^T y)
\end{align}

And $f(t) = Mc(t)$ follows the separable ODE:
\begin{align}
    \partial_t f(t) &= M \partial_t c(t)\\
                    &= M M^T y - M M^T M c\\
                    &= K y - K Mc \\
                    &= K y - K f(t)
\end{align}

whose solutions are of the form:
\begin{equation}\label{eq:f_dynamics}
    f(t) \propto \exp(-K) \mathbbm{1} t + y
\end{equation}

We can decompose $K$ into its eigenbasis where $\mathbf{e_i}$, an eigenvector of $K$ and $\lambda_i$ is the corresponding eigenvalue for $i = 1, \ldots, s$. 

We can then write \eqref{eq:f_dynamics} as:
\begin{align}
    f(t) = y + \sum_{i=1}^n exp(-t \lambda_i \mathbf{e_i})
\end{align}

which implies the dynamics of the residual, $f(t) - y$ are:
\begin{equation}\label{eq:eigenkernel}
    f(t) - y = \sum_{i=1}^n exp(-t \lambda_i \mathbf{e_i})
\end{equation}

Thus, as $t \rightarrow \infty$, the terms $exp(-t \lambda_i\mathbf{e_i})$ decay, at a rate exponential in $\lambda_i$ and $f(t) \rightarrow y$, which, as mentioned in \cite{NTKJacot}, suggests that early stopping acts as a regularizer by decaying the residual primarily along the principle components of the kernel. These principle components usually correspond to smoother  Furthermore, \eqref{eq:eigenkernel} means that if the kernel is not full rank, then the residual cannot go to zero since some $\lambda_i$ will be zero.

\subsection{Avoiding Gradient Descent}

In practice when the kernel is fixed, we can avoid gradient descent altogether and solve the least squares problem using a pseudo-inverse. For non-degenerate kernels, solutions using this method will be interpolatory which is not always desirable in the case of noisy data. We can still exploit the implicit regularization properties discussed in the previous section by modifying the least squares problem to:

\begin{equation}
    \text{minimize } ||(M + \epsilon I^{s \times m})c - y||^2
\end{equation},

which has the effect of discounting the principle components of $M$ with small singular values.

\subsubsection{Eigenvectors of Adaptive Kernel}
In the non-lazy case, we expect the kernels to degenerate as knots pile up at attractor points. The locations of attractor nodes provide a form of implicit regularization as they bias the function, $f(x)$ towards piecewise linear functions with piece boundaries lying at the attractor nodes.

In the extreme of the non-lazy case, all the knots will concentrate on input samples. Multiple overlapping knots will reduce the rank of the Kernel to the number of samples. We can simplify this scenario by considering a kernel with one knot per sample.

Recall, that we can parameterize $f$ in terms of knots $e_i = \frac{-b_i}{a_i}$. Without loss of generality, assume that for all $i$, $a_i = 1$. Now assume that there is one knot per sample, $x_i$, i.e. $x_i = -b_i$. Then we can write $f$ as:

\begin{equation}
    f(x) = \sum_{i=1}^m c_i [x + x_i]_+
\end{equation}

and we can write the kernel $K$ as
\begin{align}
    K_{ij} &= \sum_{l=1}^m [x_i - b_l]_+ [x_j - b_l]_+\\
           &= \sum_{l=1}^m [x_i - x_l]_+ [x_j - x_l]_+\\
           &= \sum_{l = 1}^{\min(x_i, x_j)-1} (x_i - x_l) (x_j - x_l)
\end{align}

Since $K$ is symmetric positive definite, we can decompose it into matrices $LDL^T$ where $L$ is a lower triangular matrix and $D$ is a diagonal matrix where 
\begin{equation*}
    D_{ii} = \begin{cases}\frac{1}{x_i - x_{i-1}}& \text{if } i > 0\\0 & \text{else}\end{cases}
\end{equation*}

We can orthonormalize the matrix $L$ to get a to get an eigendecomposition of the form $K = U \Lambda U^T$ using givens rotations and appropriate scaling of the rows:

\todo{TODO: Orthogonalize $L$ and compute the appropriate scaling of $D$ to get the correct eigenvalues.}
